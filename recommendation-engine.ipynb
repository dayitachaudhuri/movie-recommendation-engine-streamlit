{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMMENDATION ENGINE:\n",
    "\n",
    "### Notebook Contents:\n",
    "\n",
    "- Content-Based Filtering (and Popularity Based Recommendation) using TMDB 5000 Dataset\n",
    "- Collaborative Filtering using MovieLens Dataset\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Importing the Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=pd.read_csv('dataset/tmdb_5000_movies.csv')\n",
    "credits=pd.read_csv('dataset/tmdb_5000_credits.csv')\n",
    "links=pd.read_csv('dataset/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Preprocessing the Imported Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tmdb_5000_movies.csv and tmdb_5000_credits.csv on the basis of title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies.merge(credits, on='title')\n",
    "movies=movies.merge(links, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the relevant columns and remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies[['id','title','overview','genres','keywords','cast','crew','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting genres and keywords fields to list format to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert1(item):\n",
    "    arr=[]\n",
    "    for i in ast.literal_eval(item):\n",
    "        arr.append(i['name'])\n",
    "    return arr\n",
    "\n",
    "movies['genres']=movies['genres'].apply(convert1)\n",
    "movies['keywords']=movies['keywords'].apply(convert1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting cast field to list format and reducing data to the name of the first 5 most significant cast members, to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2(item):\n",
    "    arr=[]\n",
    "    count=0\n",
    "    for i in ast.literal_eval(item):\n",
    "        if count!=5:\n",
    "            arr.append(i['name'])\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "    return arr\n",
    "\n",
    "movies['cast']=movies['cast'].apply(convert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Director's details from crew field and rejecting other insignificant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert3(obj):\n",
    "    L=[]\n",
    "    for i in ast.literal_eval(obj):\n",
    "        if i['job']=='Director':\n",
    "            L.append(i['name'])\n",
    "            break\n",
    "    return L\n",
    "\n",
    "movies['crew']=movies['crew'].apply(convert3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing white spaces from keywords, cast and crew fields to avoid discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['keywords']=movies['keywords'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['cast']=movies['cast'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['crew']=movies['crew'].apply(lambda x: [i.replace(\" \",\"\") for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the string in overview field to a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['overview']=movies['overview'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the overview, genres, keywords, cast and crew fields into a single tags fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['tags']=movies['overview']+movies['genres']+movies['keywords']+movies['cast']+movies['crew']\n",
    "new_df=movies[['id','title','tags','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the list of strings in tags field into one single string that can be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_29552\\2380013068.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_29552\\2380013068.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
    "new_df['tags']=new_df['tags'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorisation - Using the Natural Language Toolkit, we reduce all words to stem words. This is necessary to ensure that similar words are processed by the model as one entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_29552\\2139505822.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(stem)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)\n",
    "\n",
    "new_df['tags']=new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer class from sklearn to vectorise the tags. Vector Similarity Matrix using cosine_similarity as a similarity measure between Movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cv=CountVectorizer(max_features=5000, stop_words='english')\n",
    "vectors=cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "movie_similarity=cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function recommend which takes a movie name as input and returns the top 5 closest vectors (similar movies) to it. This will be our blueprint for all recommend functions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index=(new_df[new_df['title']==movie].index[0])\n",
    "    distances=movie_similarity[movie_index]\n",
    "    movies_list=sorted(list(enumerate(distances)), reverse=True, key=lambda x:x[1])[1:6]\n",
    "\n",
    "    for i in movies_list:\n",
    "        print(new_df.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(new_df.to_dict(),open('movies.pkl','wb'))\n",
    "pickle.dump(movie_similarity,open('recommend_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the user ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('dataset/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training data and testing data, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(ratings, test_size = 0.30, random_state = 42)\n",
    "data = x_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of train and test datasets. \n",
    "\n",
    "These dummy datasets will be used to check whether user has given rating. The movies not rated by user is marked as 1 for prediction. The movies not rated by user is marked as 0 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = x_train.copy()\n",
    "dummy_test = x_test.copy()\n",
    "\n",
    "dummy_train['rating'] = dummy_train['rating'].apply(lambda x: 0 if x > 0 else 1)\n",
    "dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = dummy_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(1)\n",
    "dummy_test = dummy_test.pivot(index ='userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Similarity Matrix using Cosine similarity as a similarity measure between Users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_similarity = cosine_similarity(data)\n",
    "user_similarity[np.isnan(user_similarity)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to recommend the same movie that the user already watched. We will ignore the movies rated by the user and we will use our dummy training matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings = np.dot(user_similarity, data)\n",
    "user_final_ratings = np.multiply(user_predicted_ratings, dummy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created User Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_final_ratings,open('recommend_2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate for the movies already rated by the User."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.07126637 ... 0.0749648  0.         0.02105064]\n",
      " [0.         1.         0.         ... 0.02631254 0.         0.04691426]\n",
      " [0.07126637 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.0749648  0.02631254 0.         ... 1.         0.06079015 0.12466251]\n",
      " [0.         0.         0.         ... 0.06079015 1.         0.02233952]\n",
      " [0.02105064 0.04691426 0.         ... 0.12466251 0.02233952 1.        ]]\n",
      "- - - - - - - - - - \n",
      "(610, 610)\n"
     ]
    }
   ],
   "source": [
    "test_user_features = x_test.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)\n",
    "test_user_similarity = cosine_similarity(test_user_features)\n",
    "test_user_similarity[np.isnan(test_user_similarity)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.01521825,  3.22701218,  1.71422693, ...,  0.04154912,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.64920152,  0.91304857,  0.02113666, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.07587801,  0.07241296,  0.1867716 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [17.86102484, 10.1363879 ,  4.48304633, ...,  0.0274908 ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 3.10351661,  2.6934212 ,  1.20357903, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [12.36110509,  5.79632466,  1.96280959, ...,  0.        ,\n",
       "         0.20526264,  0.23947308]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_predicted_ratings_test = np.dot(test_user_similarity, test_user_features)\n",
    "test_user_final_rating = np.multiply(user_predicted_ratings_test, dummy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the final ratings in range 0.5 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[       nan        nan        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " ...\n",
      " [       nan 2.28631493        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]\n",
      " [       nan        nan        nan ...        nan        nan        nan]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = test_user_final_rating.copy() \n",
    "X = X[X > 0]\n",
    "scaler = MinMaxScaler(feature_range = (0.5, 5))\n",
    "scaler.fit(X)\n",
    "pred = scaler.transform(X)\n",
    "\n",
    "total_non_nan = np.count_nonzero(~np.isnan(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test.pivot(index = 'userId', columns = 'movieId', values = 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5635654266606624\n"
     ]
    }
   ],
   "source": [
    "diff_sqr_matrix = (test - pred)**2\n",
    "sum_of_squares_err = diff_sqr_matrix.sum().sum() \n",
    "\n",
    "rmse = np.sqrt(sum_of_squares_err/total_non_nan)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2116342196650216\n"
     ]
    }
   ],
   "source": [
    "mae = np.abs(pred - test).sum().sum()/total_non_nan\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can Create a deviation of the MAE to make our model more accurate."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbc768028c3e6ead51d9a200ddcb2ec858ae62844dcd1994729a8279be9b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
