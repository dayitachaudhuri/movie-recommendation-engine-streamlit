{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMMENDATION ENGINE:\n",
    "\n",
    "### Notebook Contents:\n",
    "\n",
    "- Content-Based Filtering (and Popularity Based Recommendation) using TMDB 5000 Dataset\n",
    "- Collaborative Filtering using MovieLens Dataset\n",
    "- Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Importing the Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=pd.read_csv('dataset/tmdb_5000_movies.csv')\n",
    "credits=pd.read_csv('dataset/tmdb_5000_credits.csv')\n",
    "links=pd.read_csv('dataset/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Preprocessing the Imported Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tmdb_5000_movies.csv and tmdb_5000_credits.csv on the basis of title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies.merge(credits, on='title')\n",
    "movies=movies.merge(links, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the relevant columns and remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies[['id','title','overview','genres','keywords','cast','crew','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting genres and keywords fields to list format to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert1(item):\n",
    "    arr=[]\n",
    "    for i in ast.literal_eval(item):\n",
    "        arr.append(i['name'])\n",
    "    return arr\n",
    "\n",
    "movies['genres']=movies['genres'].apply(convert1)\n",
    "movies['keywords']=movies['keywords'].apply(convert1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting cast field to list format and reducing data to the name of the first 5 most significant cast members, to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2(item):\n",
    "    arr=[]\n",
    "    count=0\n",
    "    for i in ast.literal_eval(item):\n",
    "        if count!=5:\n",
    "            arr.append(i['name'])\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "    return arr\n",
    "\n",
    "movies['cast']=movies['cast'].apply(convert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Director's details from crew field and rejecting other insignificant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert3(obj):\n",
    "    L=[]\n",
    "    for i in ast.literal_eval(obj):\n",
    "        if i['job']=='Director':\n",
    "            L.append(i['name'])\n",
    "            break\n",
    "    return L\n",
    "\n",
    "movies['crew']=movies['crew'].apply(convert3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing white spaces from keywords, cast and crew fields to avoid discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['keywords']=movies['keywords'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['cast']=movies['cast'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['crew']=movies['crew'].apply(lambda x: [i.replace(\" \",\"\") for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the string in overview field to a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['overview']=movies['overview'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the overview, genres, keywords, cast and crew fields into a single tags fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['tags']=movies['overview']+movies['genres']+movies['keywords']+movies['cast']+movies['crew']\n",
    "new_df=movies[['id','title','tags','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the list of strings in tags field into one single string that can be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13600\\2380013068.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13600\\2380013068.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
    "new_df['tags']=new_df['tags'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorisation - Using the Natural Language Toolkit, we reduce all words to stem words. This is necessary to ensure that similar words are processed by the model as one entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13600\\2139505822.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(stem)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)\n",
    "\n",
    "new_df['tags']=new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer class from sklearn to vectorise the tags. Vector Similarity Matrix using cosine_similarity as a similarity measure between Movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cv=CountVectorizer(max_features=5000, stop_words='english')\n",
    "vectors=cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "movie_similarity=cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function recommend which takes a movie name as input and returns the top 5 closest vectors (similar movies) to it. This will be our blueprint for all recommend functions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index=(new_df[new_df['title']==movie].index[0])\n",
    "    distances=movie_similarity[movie_index]\n",
    "    movies_list=sorted(list(enumerate(distances)), reverse=True, key=lambda x:x[1])[1:6]\n",
    "\n",
    "    for i in movies_list:\n",
    "        print(new_df.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(new_df.to_dict(),open('movies.pkl','wb'))\n",
    "pickle.dump(movie_similarity,open('recommend_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. User-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the user ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('dataset/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training data and testing data, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(ratings, test_size = 0.30, random_state = 42)\n",
    "data = x_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of train and test datasets. \n",
    "\n",
    "These dummy datasets will be used to check whether user has given rating. The movies not rated by user is marked as 1 for prediction. The movies not rated by user is marked as 0 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = x_train.copy()\n",
    "dummy_test = x_test.copy()\n",
    "\n",
    "dummy_train['rating'] = dummy_train['rating'].apply(lambda x: 0 if x > 0 else 1)\n",
    "dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = dummy_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(1)\n",
    "dummy_test = dummy_test.pivot(index ='userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Similarity Matrix using Cosine similarity as a similarity measure between Users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_similarity = cosine_similarity(data)\n",
    "user_similarity[np.isnan(user_similarity)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to recommend the same movie that the user already watched. We will ignore the movies rated by the user and we will use our dummy training matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings = np.dot(user_similarity, data)\n",
    "user_final_ratings = np.multiply(user_predicted_ratings, dummy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created User Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_final_ratings,open('recommend_2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Item-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_features = x_train.pivot(index = 'movieId', columns = 'userId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "item_similarity = cosine_similarity(movie_features)\n",
    "item_similarity[np.isnan(item_similarity)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings = np.dot(movie_features.T, item_similarity)\n",
    "item_final_ratings = np.multiply(item_predicted_ratings, dummy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>190221</th>\n",
       "      <th>191005</th>\n",
       "      <th>193565</th>\n",
       "      <th>193571</th>\n",
       "      <th>193573</th>\n",
       "      <th>193579</th>\n",
       "      <th>193581</th>\n",
       "      <th>193583</th>\n",
       "      <th>193585</th>\n",
       "      <th>193609</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>67.074925</td>\n",
       "      <td>78.342161</td>\n",
       "      <td>51.056146</td>\n",
       "      <td>32.737629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.10717</td>\n",
       "      <td>46.727041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.888433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 8566 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId     1          2          3          4       5         6       \\\n",
       "userId                                                                  \n",
       "43       67.074925  78.342161  51.056146  32.737629     0.0  45.10717   \n",
       "\n",
       "movieId     7       8          9       10      ...    190221  191005  193565  \\\n",
       "userId                                         ...                             \n",
       "43       46.727041     0.0  17.888433     0.0  ...  0.207754     0.0     0.0   \n",
       "\n",
       "movieId  193571  193573  193579  193581  193583  193585    193609  \n",
       "userId                                                             \n",
       "43          0.0     0.0     0.0     0.0     0.0     0.0  0.354246  \n",
       "\n",
       "[1 rows x 8566 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_final_ratings.iloc[42:43].sort_values(by=43,ascending = False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For User-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate for the movies already rated by the User."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_features = x_test.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)\n",
    "test_user_similarity = cosine_similarity(test_user_features)\n",
    "test_user_similarity[np.isnan(test_user_similarity)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings_test = np.dot(test_user_similarity, test_user_features)\n",
    "test_user_final_rating = np.multiply(user_predicted_ratings_test, dummy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the final ratings in range 0.5 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = test_user_final_rating.copy() \n",
    "X = X[X > 0]\n",
    "scaler = MinMaxScaler(feature_range = (0.5, 5))\n",
    "scaler.fit(X)\n",
    "pred = scaler.transform(X)\n",
    "\n",
    "total_non_nan = np.count_nonzero(~np.isnan(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = x_test.pivot(index = 'userId', columns = 'movieId', values = 'rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Root Mean Square Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5635654266606624\n"
     ]
    }
   ],
   "source": [
    "diff_sqr_matrix = (test - pred)**2\n",
    "sum_of_squares_err = diff_sqr_matrix.sum().sum() \n",
    "\n",
    "rmse = np.sqrt(sum_of_squares_err/total_non_nan)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2116342196650216\n"
     ]
    }
   ],
   "source": [
    "mae = np.abs(pred - test).sum().sum()/total_non_nan\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Item-Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_features = x_test.pivot(index = 'movieId', columns = 'userId', values = 'rating').fillna(0)\n",
    "test_item_similarity = cosine_similarity(test_item_features)\n",
    "test_item_similarity[np.isnan(test_item_similarity)] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_predicted_ratings_test = np.dot(test_item_features.T, test_item_similarity )\n",
    "test_item_final_rating = np.multiply(item_predicted_ratings_test, dummy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = test_item_final_rating.copy() \n",
    "X = X[X > 0]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0.5, 5))\n",
    "scaler.fit(X)\n",
    "pred = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_non_nan = np.count_nonzero(~np.isnan(pred))\n",
    "test = x_test.pivot(index = 'userId', columns = 'movieId', values = 'rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.512699212653213\n"
     ]
    }
   ],
   "source": [
    "diff_sqr_matrix = (test - pred)**2\n",
    "sum_of_squares_err = diff_sqr_matrix.sum().sum()\n",
    "\n",
    "rmse = np.sqrt(sum_of_squares_err/total_non_nan)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.215407217950911\n"
     ]
    }
   ],
   "source": [
    "mae = np.abs(pred - test).sum().sum()/total_non_nan\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the RMSE and MAE values, we can conclude that User-Based Collaborative Filtering is more efficient than Item-Based Collaborative Filtering. Hence we have implemented User-Based Collaborative Filtering in teh web application."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbc768028c3e6ead51d9a200ddcb2ec858ae62844dcd1994729a8279be9b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
