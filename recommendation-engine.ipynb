{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMMENDATION ENGINE:\n",
    "\n",
    "### Notebook Contents:\n",
    "\n",
    "- Content-Based Filtering using TMDB 5000 Dataset\n",
    "- Collaborative Filtering using MovieLens Dataset\n",
    "- Popularity based recommendation\n",
    "- Evaluation\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Content-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Importing the Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=pd.read_csv('dataset/tmdb_5000_movies.csv')\n",
    "credits=pd.read_csv('dataset/tmdb_5000_credits.csv')\n",
    "links=pd.read_csv('dataset/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Preprocessing the Imported Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging tmdb_5000_movies.csv and tmdb_5000_credits.csv on the basis of title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies.merge(credits, on='title')\n",
    "movies=movies.merge(links, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the relevant columns and remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=movies[['id','title','overview','genres','keywords','cast','crew','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting genres and keywords fields to list format to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert1(item):\n",
    "    arr=[]\n",
    "    for i in ast.literal_eval(item):\n",
    "        arr.append(i['name'])\n",
    "    return arr\n",
    "\n",
    "movies['genres']=movies['genres'].apply(convert1)\n",
    "movies['keywords']=movies['keywords'].apply(convert1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting cast field to list format and reducing data to the name of the first 5 most significant cast members, to make processing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2(item):\n",
    "    arr=[]\n",
    "    count=0\n",
    "    for i in ast.literal_eval(item):\n",
    "        if count!=5:\n",
    "            arr.append(i['name'])\n",
    "            count+=1\n",
    "        else:\n",
    "            break\n",
    "    return arr\n",
    "\n",
    "movies['cast']=movies['cast'].apply(convert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Director's details from crew field and rejecting other insignificant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert3(obj):\n",
    "    L=[]\n",
    "    for i in ast.literal_eval(obj):\n",
    "        if i['job']=='Director':\n",
    "            L.append(i['name'])\n",
    "            break\n",
    "    return L\n",
    "\n",
    "movies['crew']=movies['crew'].apply(convert3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing white spaces from keywords, cast and crew fields to avoid discrepancies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['keywords']=movies['keywords'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['cast']=movies['cast'].apply(lambda x: [i.replace(\" \",\"\") for i in x])\n",
    "movies['crew']=movies['crew'].apply(lambda x: [i.replace(\" \",\"\") for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the string in overview field to a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['overview']=movies['overview'].apply(lambda x: str(x).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating the overview, genres, keywords, cast and crew fields into a single tags fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['tags']=movies['overview']+movies['genres']+movies['keywords']+movies['cast']+movies['crew']\n",
    "new_df=movies[['id','title','tags','popularity','vote_average','movieId']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the list of strings in tags field into one single string that can be used for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_27080\\2380013068.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_27080\\2380013068.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "new_df['tags']=new_df['tags'].apply(lambda x: \" \".join(x))\n",
    "new_df['tags']=new_df['tags'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorisation - Using the Natural Language Toolkit, we reduce all words to stem words. This is necessary to ensure that similar words are processed by the model as one entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_27080\\2139505822.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['tags']=new_df['tags'].apply(stem)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    y=[]\n",
    "    for i in text.split():\n",
    "        y.append(ps.stem(i))\n",
    "    return \" \".join(y)\n",
    "\n",
    "new_df['tags']=new_df['tags'].apply(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer class from sklearn to vectorise the tags. Vector Similarity Matrix using cosine_similarity as a similarity measure between Movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cv=CountVectorizer(max_features=5000, stop_words='english')\n",
    "vectors=cv.fit_transform(new_df['tags']).toarray()\n",
    "\n",
    "movie_similarity=cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function recommend which takes a movie name as input and returns the top 5 closest vectors (similar movies) to it. This will be our blueprint for all recommend functions in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(movie):\n",
    "    movie_index=(new_df[new_df['title']==movie].index[0])\n",
    "    distances=movie_similarity[movie_index]\n",
    "    movies_list=sorted(list(enumerate(distances)), reverse=True, key=lambda x:x[1])[1:6]\n",
    "\n",
    "    for i in movies_list:\n",
    "        print(new_df.iloc[i[0]].title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(new_df.to_dict(),open('movies.pkl','wb'))\n",
    "pickle.dump(movie_similarity,open('recommend_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the user ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('dataset/ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training data and testing data, and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test = train_test_split(ratings, test_size = 0.30, random_state = 42)\n",
    "data = x_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of train and test datasets. \n",
    "\n",
    "These dummy datasets will be used to check whether user has given rating. The movies not rated by user is marked as 1 for prediction. The movies not rated by user is marked as 0 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = x_train.copy()\n",
    "dummy_test = x_test.copy()\n",
    "\n",
    "dummy_train['rating'] = dummy_train['rating'].apply(lambda x: 0 if x > 0 else 1)\n",
    "dummy_test['rating'] = dummy_test['rating'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_train = dummy_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(1)\n",
    "dummy_test = dummy_test.pivot(index ='userId', columns = 'movieId', values = 'rating').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Similarity Matrix using Cosine similarity as a similarity measure between Users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "user_similarity = cosine_similarity(data)\n",
    "user_similarity[np.isnan(user_similarity)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to recommend the same movie that the user already watched. We will ignore the movies rated by the user and we will use our dummy training matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_predicted_ratings = np.dot(user_similarity, data)\n",
    "user_final_ratings = np.multiply(user_predicted_ratings, dummy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pickle to create pickle files and storing our created User Similarity Matrix. Our application will be able to derive closest vectors during recommendation from these pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(user_final_ratings,open('recommend_2.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbc768028c3e6ead51d9a200ddcb2ec858ae62844dcd1994729a8279be9b48f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
